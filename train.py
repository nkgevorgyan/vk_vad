import torch.optim as optim
from IPython.display import clear_output
import time
import model
import torch 
import os 
import numpy as np
import h5py_cache
from data_load import get_dataset

import torch.nn as nn
from torch.nn import Linear, RNN, LSTM, GRU
import torch.nn.functional as F
from torch.nn.functional import softmax, relu
from torch.autograd import Variable

NOISE_LEVELS = ['None', '-15', '-3']

DATA_FOLDER = 'data'

def accuracy(out, y):
    '''
    Calculate accuracy of model where
    out.shape = (64, 2) and y.shape = (64)
    '''
    out = torch.max(out, 1)[1].float()
    eq = torch.eq(out, y.float()).float()
    return torch.mean(eq)

def net_path(epoch, title):
    part = os.getcwd() + '/models/' + title
    if epoch >= 0:
        return part + '_epoch' + str(epoch).zfill(3) + '.net'
    else:
        return part + '.net'


def save_net(net, epoch, title='net'):
    if not os.path.exists(os.getcwd() + '/models'):
        os.makedirs(os.getcwd() + '/models')
    torch.save(net, net_path(epoch, title))


def load_net(epoch=14, title='net'):
    if torch.cuda.is_available():
        return torch.load(net_path(epoch, title))
    else:
        return torch.load(net_path(epoch, title), map_location='cpu')


def train_net(net, epochs=15, lr=1e-3, weight_decay=1e-5,
              momentum=0.9, early_stopping=False, patience=25,
              frame_count=model.FRAMES, step_size=model.STEP_SIZE, auto_save=True, title='net'):
    '''
    Full-featured training of a given neural network.
    A number of training parameters are optionally adjusted.
    If verbose is True, the training progress is continously
    plotted at the end of each epoch.
    If auto_save is True, the model will be saved every epoch.
    '''

    # Set up an instance of data generator using default partitions

    train_set, val_set, _ = get_dataset(NOISE_LEVELS)

    # Instantiate the chosen loss function
    criterion = nn.CrossEntropyLoss()
    levels = NOISE_LEVELS

    # Move criterion to GPU if available
    if torch.cuda.is_available():
        net.cuda()
        criterion.cuda()

    # Instantiate the chosen optimizer with the parameters specified
    optimizer = optim.Adam(net.parameters(), lr=lr, weight_decay=weight_decay)

    net.train()
    stalecount, maxacc = 0, 0

    def run(net, dataset, optimize=False):
        '''
        This function constitutes a single epoch.
        Snippets are loaded into memory and their associated
        frames are loaded as generators. As training progresses
        and new frames are needed, they are generated by the iterator,
        and are thus not stored in memory when not used.
        If optimize is True, the associated optimizer will backpropagate
        and adjust network weights.
        Returns the average sample loss and accuracy for that epoch.
        '''
        epoch_loss, epoch_acc, level_acc = 0, [], []
        batches = dataset[0].batch_count
        num_batches = batches * len(levels)
        # In case we apply focal loss, we want to include all noise levels

        # Helper function responsible for running a batch
        def run_batch(X, y, epoch_loss, epoch_acc):

            X = Variable(torch.from_numpy(np.array(X)).float())
            y = Variable(torch.from_numpy(np.array(y))).long()

            if torch.cuda.is_available():
                X = X.cuda()
                y = y.cuda()

            out = net(X)

            # Compute loss and accuracy for batch
            batch_loss = criterion(out, y)
            batch_acc = accuracy(out, y)

            # If training session, initiate backpropagation and optimization
            if optimize == True:
                optimizer.zero_grad()
                batch_loss.backward()
                optimizer.step()

            if torch.cuda.is_available():
                batch_acc = batch_acc.cpu()
                batch_loss = batch_loss.cpu()

            # Accumulate loss and accuracy for epoch metrics
            epoch_loss += batch_loss.data.numpy() / float(model.BATCH_SIZE)
            epoch_acc.append(batch_acc.data.numpy())

            return epoch_loss, epoch_acc

        # For each noise level scheduled
        for level in range(len(levels)):
            print("Dataset with noise {}".format(level))

            # For each batch in noise level
            for idx in range(batches):
                print('{} batch out of {} batches'.format(idx, batches))
                # Get a new batch and run it
                X, y = dataset[level][idx]
                temp_loss, temp_acc = run_batch(X, y, epoch_loss, epoch_acc)
                epoch_loss += temp_loss / float(num_batches)
                level_acc.append(np.mean(temp_acc))

        return epoch_loss, np.mean(level_acc)

    losses, accs, val_losses, val_accs = [], [], [], []


    # Iterate over training epochs
    for epoch in range(epochs):

        print('========================= Epoch {} ========================'.format(epoch))
        # Calculate loss and accuracy for that epoch and optimize
        loss, acc = run(net, train_set, optimize=True)
        losses.append(loss)
        accs.append(acc)

        net.eval()
        val_loss, val_acc = run(net, val_set)
        # print(val_loss, val_acc)
        # return
        val_losses.append(val_loss)
        val_accs.append(val_acc)
        net.train()

        # Early stopping algorithm.
        # If validation accuracy does not improve for
        # a set amount of epochs, abort training and retrieve
        # the best model (according to validation accuracy)
        if epoch > 0 and val_accs[-1] <= maxacc:
            stalecount += 1
            if stalecount > patience and early_stopping:
                return
        else:
            stalecount = 0
            maxacc = val_accs[-1]

        if auto_save:
            save_net(net, epoch, title)


#data = h5py_cache.File(DATA_FOLDER + '/data.hdf5', 'r')
VAD = model.VADModel(model.FEATURES, model.FRAMES, model.BATCH_SIZE)
train_net(VAD)
